\section{Analysis -- Setting the Baseline for kNN and determining k}
\label{sec:numtextons}

In a standard setting, the training error $\epsilon_t$ of a
$k$=1-nearest neighbor algorithm is $\epsilon_t = 0$ because the
nearest neighbor of the sample will be the sample itself, given that each feature vector is unique. However, in
this scenario, we deal with random sampling such that each image will
be represented by a slightly different histogram each time the
histogram is extracted.