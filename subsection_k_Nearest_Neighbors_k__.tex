\subsection{$k$-Nearest Neighbors ($k$NN) algorithm}

% TODO: Write that one could also use a more efficient data structure than simply a list

The $k$-Nearest Neighbors ($k$NN) algorithm is the machine learning-core of the proposed algorithm. For a given histogram that is derived from the current camera image, the $k$-Nearest Neighbors ($k$NN) algorithm measures the similarity of this histogram to all histograms in the training dataset and outputs the $k$ most similar training histograms and the corresponding $x,y$-positions.

%The similarity measure is a function that takes two samples as input and outputs a real value. We used the cosine similarity as similarity function, since it is bounded between 0 and 1. While other similarity measurements exists, the basic similarity measures—such as Euclidean distance—do not have a large impact on the results (Figure xxx). The choice of $k$ was based on the heuristic that XXX (TODO: write something like the sqrt of N or whatever) and on cross-validation.  
While the $k$NN algorithm is one of the simplest machine learning algorithms, it
offers several advantages: it is non-parametric, allowing for the
modeling of arbitrary distributions. Its capability to output multiple
predictions allows for neat integration with the proposed particle filter. Its simplicity comes together with transparency: it allows for spotting the possible sources of error such as wrongly labeled training examples.