\subsection{$k$-Nearest Neighbors ($k$NN) algorithm}

% TODO: Write that one could also use a more efficient data structure than simply a list

The $k$-Nearest Neighbors ($k$NN) algorithm is the machine learning-core of the proposed algorithm. For a given histogram that is derived from the current camera image, the $k$-Nearest Neighbors ($k$NN) algorithm measures the similarity of this histogram to all histograms in the training dataset and outputs the $k$ most similar training histograms and the corresponding $x,y$-positions.

%The similarity measure is a function that takes two samples as input and outputs a real value. We used the cosine similarity as similarity function, since it is bounded between 0 and 1. While other similarity measurements exists, the basic similarity measures—such as Euclidean distance—do not have a large impact on the results (Figure xxx). The choice of $k$ was based on the heuristic that XXX (TODO: write something like the sqrt of N or whatever) and on cross-validation.  
While the $k$NN algorithm is one of the simplest machine learning algorithms, it
offers several advantages: it is non-parametric, allowing for the
modeling of arbitrary distributions. Its capability to output multiple
predictions allows for neat integration with the proposed particle filter. Its simplicity comes together with transparency: it allows for spotting the possible sources of error such as wrongly labeled training examples.

While the naive approach in using $k$NN for regression calculates the mean of the $k$ outputs, we decided to use a more complex method. This motivation is visualized in Figure XXX: If $k=2$ and the output values are distant to each other, averaging them would yield a value in the middle, which is with high certainty not the correct position. Over time, however, the ambiguity, can be resolved, when both estimates of the $k$NN model fall together. Compared to the Kalman filter, which is displayed in Figure XXX, the full Bayesian filter can immediately find the correct position. Since a full Bayesian filter is computationally complex, a variant that is based on Monte Carlo sampling was used: the particle filter. A more detailed description of the filtering technique can be found in the next section.  

% TODO: Compare particle filter with Kalman filter!!   

It often outperforms more sophisticated algorithms. A frequent critique regarding the $k$NN is its increasing computational complexity with an increasing size of the training dataset. However, its time complexity can be reduced by storing the training examples in an efficient manner, such as a binary tree structure. However, all of our training datasets were below 1000 images, resulting in fast predictions based on a list structure. 