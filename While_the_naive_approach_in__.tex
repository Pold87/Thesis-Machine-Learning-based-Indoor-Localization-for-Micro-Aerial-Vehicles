While the naive approach in using $k$NN for regression calculates the mean of the $k$ outputs, we decided to use a more complex method. This motivation is visualized in Figure XXX: If $k=2$ and the output values are distant to each other, averaging them would yield a value in the middle, which is with high certainty not the correct position. Over time, however, the ambiguity, can be resolved, when both estimates of the $k$NN model fall together. Compared to the Kalman filter, which is displayed in Figure XXX, the full Bayesian filter can immediately find the correct position. Since a full Bayesian filter is computationally complex, a variant that is based on Monte Carlo sampling was used: the particle filter. A more detailed description of the filtering technique can be found in the next section.  

% TODO: Compare particle filter with Kalman filter!!   

It often outperforms more sophisticated algorithms. A frequent critique regarding the $k$NN is its increasing computational complexity with an increasing size of the training dataset. However, its time complexity can be reduced by storing the training examples in an efficient manner, such as a binary tree structure. However, all of our training datasets were below 1000 images, resulting in fast predictions based on a list structure. 